{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow to create the good data organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data INPUT to be written to a JSON file\n",
    "name_architectures = [\n",
    "    'Clements_Arct',\n",
    "    'Fldzhyan_Arct',\n",
    "    'NEUROPULSCrossingSide_Arct']\n",
    "\n",
    "arct = 'Fldzhyan_Arct'\n",
    "\n",
    "n_inputs = 32\n",
    "if n_inputs == 4:\n",
    "    n_epochs = 20000\n",
    "elif n_inputs == 6:\n",
    "    n_epochs = 20000 + 1000\n",
    "elif n_inputs == 8:\n",
    "    n_epochs = 20000 + 2000\n",
    "elif n_inputs == 10:\n",
    "    n_epochs = 20000 + 3000\n",
    "elif n_inputs == 12:\n",
    "    n_epochs = 20000 + 4000\n",
    "elif n_inputs == 14:\n",
    "    n_epochs = 20000 + 5000\n",
    "elif n_inputs == 16:\n",
    "    n_epochs = 20000 + 6000\n",
    "elif n_inputs == 18:\n",
    "    n_epochs = 20000 + 7000\n",
    "elif n_inputs == 20:\n",
    "    n_epochs = 20000 + 8000\n",
    "elif n_inputs == 24:\n",
    "    n_epochs = 20000 + 10000\n",
    "elif n_inputs == 28:\n",
    "    n_epochs = 20000 + 12000\n",
    "elif n_inputs == 32:\n",
    "    n_epochs = 20000 + 14000\n",
    "\n",
    "directory_run = 'outdata/20241204_run_18_gausN'\n",
    "\n",
    "name_file = \"traking_fidelities_\"+arct+\"_N\"+str(n_inputs)\n",
    "\n",
    "data = {\n",
    "    \"name_file\": name_file,\n",
    "    \"arct\": arct,\n",
    "    \"n_inputs\": n_inputs,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"date\": \"20240902\",\n",
    "    \"train_type\": \"1-Fidelity\",\n",
    "    \"seed\": 37,\n",
    "    \"n_matrices\": 1000,\n",
    "    \"n_repetitions\": 5,\n",
    "    \"lr\": 0.001,\n",
    "    \"n_bachup\": 500,\n",
    "\n",
    "    \"data_out_type\": \"Fidelity\",\n",
    "    \"rep_type\": \"max\",\n",
    "    \"simulations\": []\n",
    "    }\n",
    "\n",
    "folder_relative_path = \"organized_data/\"\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_gaussian_output_simulation(data,\n",
    "                                      ilPhSm, ilPhSs,\n",
    "                                      ilMMIm, ilMMIs, imbMMIm, imbMMIs,\n",
    "                                      ilCROSm, ilCROSs, ctCROSm, ctCROSs, fidelities=None):\n",
    "    dictionary = {\n",
    "        \"pc_iloss_mu\": ilPhSm,\n",
    "        \"pc_iloss_sigma\": ilPhSs,\n",
    "        \"iloss_MMI_mu\": ilMMIm,\n",
    "        \"iloss_MMI_sigma\": ilMMIs,\n",
    "        \"imbalance_mu\": imbMMIm,\n",
    "        \"imbalance_sigma\": imbMMIs,\n",
    "        \"iloss_cross_mu\": ilCROSm,\n",
    "        \"iloss_cross_sigma\": ilCROSs,\n",
    "        \"ct_cross_mu\": ctCROSm,\n",
    "        \"ct_cross_sigma\": ctCROSs,\n",
    "\n",
    "        \"fidelities\": fidelities\n",
    "        }\n",
    "    \n",
    "    data[\"simulations\"].append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive the Average and Std Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data saved: 5000\n"
     ]
    }
   ],
   "source": [
    "# derive Fidelity for the one simulation\n",
    "# Compute Fidelity Unitary\n",
    "def FidelityUnitary(target_matrix, predicted_matrix):\n",
    "    target_matrix = np.array(target_matrix, dtype=np.complex128)\n",
    "    predicted_matrix = np.array(predicted_matrix, dtype=np.complex128)\n",
    "    trace_pH_p = np.trace(np.dot(predicted_matrix.conj().T, predicted_matrix)).real    # result is REAL\n",
    "    trace_pH_t = np.trace(np.dot(predicted_matrix.conj().T, target_matrix))\n",
    "    cosine_similarity = np.abs(trace_pH_t)**2/(target_matrix.shape[0]*trace_pH_p)\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "# function get the directory and condition\n",
    "# check all the files and if its good save the\n",
    "def extract_fidelities_1sim(directory, arct):\n",
    "    fidelities = []\n",
    "    labels = []\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)       # Construct the full file path\n",
    "        if os.path.isfile(file_path):                       # Check if it is a file\n",
    "            result_sim = np.load(file_path, allow_pickle=True)\n",
    "            target_predit_tuples = [(label, array) for label, array in result_sim if arct in label]\n",
    "        if target_predit_tuples:    # If there are elements\n",
    "            labels.extend([target_predit_tuples[i][0].replace(\"target_\", \"\", 1) for i in range(0, len(target_predit_tuples), 2)])\n",
    "            fidelities.extend([FidelityUnitary(target_predit_tuples[i][1], target_predit_tuples[i+1][1]) for i in range(0, len(target_predit_tuples), 2)])\n",
    "    fidelity_pd = pd.DataFrame({'label': labels, 'value': fidelities})\n",
    "    return fidelity_pd\n",
    "\n",
    "def extract_values(simulation_folder):\n",
    "    # Patterns to match each value, ensuring correct order\n",
    "    patterns = {\n",
    "        'pc_iloss_mu': r\"ilPhSm(-?\\d+\\.\\d+)\",\n",
    "        'pc_iloss_sigma': r\"ilPhSs(-?\\d+\\.\\d+)\",\n",
    "        'iloss_MMI_mu': r\"ilMMIm(-?\\d+\\.\\d+)\",\n",
    "        'iloss_MMI_sigma': r\"ilMMIs(-?\\d+\\.\\d+)\",\n",
    "        'imbalance_mu': r\"imbMMIm(-?\\d+\\.\\d+)\",\n",
    "        'imbalance_sigma': r\"imbMMIs(-?\\d+\\.\\d+)\",\n",
    "        'iloss_cross_mu': r\"ilCROSm(-?\\d+\\.\\d+)\",\n",
    "        'iloss_cross_sigma': r\"ilCROSs(-?\\d+\\.\\d+)\",\n",
    "        'ct_cross_mu': r\"ctCROSm(-?\\d+\\.\\d+)\",\n",
    "        'ct_cross_sigma': r\"ctCROSs(-?\\d+\\.\\d+)\",\n",
    "    }\n",
    "    values = {}\n",
    "    last_end = 0  # Track the end position of the last match\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, simulation_folder[last_end:])\n",
    "        if match:\n",
    "            values[key] = float(match.group(1))\n",
    "            last_end += match.end()\n",
    "        else:\n",
    "            values[key] = 0.0\n",
    "    return values\n",
    "\n",
    "# take max value and derive average and std dev\n",
    "def models_repmax_mean_devstd(df, simulation_folder):\n",
    "    # Extracting the base label (e.g., 'LabelA', 'LabelB')\n",
    "    df['label_no_rep'] = df['label'].apply(lambda x: x.split('_rep')[0])\n",
    "    # Group with the base label and find max\n",
    "    max_lastLoss_s = df.groupby('label_no_rep')['value'].max()\n",
    "    \n",
    "    # Extract and convert to float\n",
    "    extract_hyp = extract_values(simulation_folder)\n",
    "    ave_std_dev_pd = pd.DataFrame([{'simulation':simulation_folder,\n",
    "                                    'pc_iloss_mu': extract_hyp['pc_iloss_mu'],\n",
    "                                    'pc_iloss_sigma': extract_hyp['pc_iloss_sigma'],\n",
    "                                    'iloss_MMI_mu': extract_hyp['iloss_MMI_mu'],\n",
    "                                    'iloss_MMI_sigma': extract_hyp['iloss_MMI_sigma'],\n",
    "                                    'imbalance_mu': extract_hyp['imbalance_mu'],\n",
    "                                    'imbalance_sigma': extract_hyp['imbalance_sigma'],\n",
    "                                    'iloss_cross_mu': extract_hyp['iloss_cross_mu'],\n",
    "                                    'iloss_cross_sigma': extract_hyp['iloss_cross_sigma'],\n",
    "                                    'ct_cross_mu': extract_hyp['ct_cross_mu'],\n",
    "                                    'ct_cross_sigma': extract_hyp['ct_cross_sigma'],\n",
    "                                    'fidelities': max_lastLoss_s.tolist(),}])\n",
    "    return ave_std_dev_pd\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "# MAIN\n",
    "# =================================================================================================================\n",
    "data_df = pd.DataFrame()\n",
    "for filename in os.listdir(directory_run):\n",
    "    directory_path = os.path.join(directory_run, filename)       # Construct the full file path\n",
    "    if \"n\"+str(data[\"n_inputs\"]) in directory_path:             # I don't like it but ok\n",
    "        if not os.path.isfile(directory_path):\n",
    "            fidelity_pd = extract_fidelities_1sim(directory_path, data[\"arct\"])\n",
    "            print(\"Number of data saved:\", fidelity_pd.shape[0])\n",
    "            if fidelity_pd.index.tolist() != []:   # No data in that simulation\n",
    "                simulation_folder = os.path.basename(directory_path)\n",
    "                ave_std_dev_pd = models_repmax_mean_devstd(fidelity_pd, simulation_folder)\n",
    "                data_df = pd.concat([data_df, ave_std_dev_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write inside the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "    # !!!!!!!!!!!! I'm APPENDING ELEMENTS !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    append_gaussian_output_simulation(\n",
    "        data=data,\n",
    "        ilPhSm=row['pc_iloss_mu'], ilPhSs=row['pc_iloss_sigma'],\n",
    "        ilMMIm=row['iloss_MMI_mu'], ilMMIs=row['iloss_MMI_sigma'], imbMMIm=row['imbalance_mu'], imbMMIs=row['imbalance_sigma'],\n",
    "        ilCROSm=row['iloss_cross_mu'], ilCROSs=row['iloss_cross_sigma'], ctCROSm=row['ct_cross_mu'], ctCROSs=row['ct_cross_sigma'],\n",
    "        fidelities=row['fidelities'],)\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by imb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary by PC_i_loss and imbalances\n",
    "import json\n",
    "\n",
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['ct_cross_sigma'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['ct_cross_mu'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['iloss_cross_sigma'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['iloss_cross_mu'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['imbalance_sigma'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['imbalance_mu'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['iloss_MMI_sigma'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['iloss_MMI_mu'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['pc_iloss_sigma'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['pc_iloss_mu'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_NP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

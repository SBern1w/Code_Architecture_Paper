{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow to create the good data organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data INPUT to be written to a JSON file\n",
    "name_architectures = [\n",
    "    'Clements_Arct',\n",
    "    'Fldzhyan_Arct',\n",
    "    'NEUROPULSCrossingSide_Arct']\n",
    "\n",
    "arct = 'NEUROPULSCrossingSide_Arct'\n",
    "\n",
    "n_inputs = 16\n",
    "if n_inputs == 4:\n",
    "    n_epochs = 20000\n",
    "elif n_inputs == 6:\n",
    "    n_epochs = 21000\n",
    "elif n_inputs == 8:\n",
    "    n_epochs = 22000\n",
    "elif n_inputs == 10:\n",
    "    n_epochs = 23000\n",
    "elif n_inputs == 12:\n",
    "    n_epochs = 24000\n",
    "elif n_inputs == 14:\n",
    "    n_epochs = 25000\n",
    "elif n_inputs == 16:\n",
    "    n_epochs = 26000\n",
    "\n",
    "directory_run = 'outdata/20240902_run_7_PCiloss'\n",
    "\n",
    "name_file = \"traking_fidelities_\"+arct+\"_N\"+str(n_inputs)\n",
    "\n",
    "data = {\n",
    "    \"name_file\": name_file,\n",
    "    \"arct\": arct,\n",
    "    \"n_inputs\": n_inputs,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"date\": \"20240902\",\n",
    "    \"train_type\": \"1-Fidelity\",\n",
    "    \"seed\": 37,\n",
    "    \"n_matrices\": 1000,\n",
    "    \"n_repetitions\": 5,\n",
    "    \"lr\": 0.001,\n",
    "    \"n_bachup\": 500,\n",
    "\n",
    "    \"data_out_type\": \"Fidelity\",\n",
    "    \"rep_type\": \"max\",\n",
    "    \"simulations\": []\n",
    "    }\n",
    "\n",
    "folder_relative_path = \"organized_data/\"\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_output_simulation(data, PC_i_loss_const, il_mmi_const, imbalance_const, il_cross_const, cross_talk_const, fidelities=None):\n",
    "    dictionary = {\n",
    "        \"PC_i_loss_const\": PC_i_loss_const,\n",
    "        \"il_mmi_const\": il_mmi_const,\n",
    "        \"imbalance_const\": imbalance_const,\n",
    "        \"il_cross_const\": il_cross_const,\n",
    "        \"cross_talk_const\": cross_talk_const,\n",
    "        \"fidelities\": fidelities\n",
    "        }\n",
    "    \n",
    "    data[\"simulations\"].append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive the Average and Std Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data saved: 5000\n"
     ]
    }
   ],
   "source": [
    "# derive Fidelity for the one simulation\n",
    "# Compute Fidelity Unitary\n",
    "def FidelityUnitary(target_matrix, predicted_matrix):\n",
    "    target_matrix = np.array(target_matrix, dtype=np.complex128)\n",
    "    predicted_matrix = np.array(predicted_matrix, dtype=np.complex128)\n",
    "    trace_pH_p = np.trace(np.dot(predicted_matrix.conj().T, predicted_matrix)).real    # result is REAL\n",
    "    trace_pH_t = np.trace(np.dot(predicted_matrix.conj().T, target_matrix))\n",
    "    cosine_similarity = np.abs(trace_pH_t)**2/(target_matrix.shape[0]*trace_pH_p)\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "# function get the directory and condition\n",
    "# check all the files and if its good save the\n",
    "def extract_fidelities_1sim(directory, arct):\n",
    "    fidelities = []\n",
    "    labels = []\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)       # Construct the full file path\n",
    "        if os.path.isfile(file_path):                       # Check if it is a file\n",
    "            result_sim = np.load(file_path, allow_pickle=True)\n",
    "            target_predit_tuples = [(label, array) for label, array in result_sim if arct in label]\n",
    "        if target_predit_tuples != []:    # If there are elements\n",
    "            labels.extend([target_predit_tuples[i][0].replace(\"target_\", \"\", 1) for i in range(0, len(target_predit_tuples), 2)])\n",
    "            fidelities.extend([FidelityUnitary(target_predit_tuples[i][1], target_predit_tuples[i+1][1]) for i in range(0, len(target_predit_tuples), 2)])\n",
    "    fidelity_pd = pd.DataFrame({'label': labels, 'value': fidelities})\n",
    "    return fidelity_pd\n",
    "\n",
    "\n",
    "def extract_values(simulation_folder):\n",
    "    # Patterns to match each value, ensuring correct order\n",
    "    patterns = {\n",
    "        'pciloss': r\"pciloss(-?\\d+\\.\\d+)\",\n",
    "        'ilmmi': r\"ilmmi(-?\\d+\\.\\d+)\",\n",
    "        'imb': r\"imb(-?\\d+\\.\\d+)\",\n",
    "        'ilcross': r\"ilcross(-?\\d+\\.\\d+)\",\n",
    "        'crosstalk': r\"crosstalk(-?\\d+\\.\\d+)\"\n",
    "    }\n",
    "    values = {}\n",
    "    last_end = 0  # Track the end position of the last match\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, simulation_folder[last_end:])\n",
    "        if match:\n",
    "            values[key] = float(match.group(1))\n",
    "            last_end += match.end()\n",
    "    return values\n",
    "\n",
    "# take max value and derive fidelities\n",
    "def models_repmax_fidelities(df, simulation_folder):\n",
    "    # Extracting the base label (e.g., 'LabelA', 'LabelB')\n",
    "    df['label_no_rep'] = df['label'].apply(lambda x: x.split('_rep')[0])\n",
    "    # Group with the base label and find max\n",
    "    max_lastLoss_s = df.groupby('label_no_rep')['value'].max()\n",
    "    \n",
    "    # Extract and convert to float\n",
    "    extract_hyp = extract_values(simulation_folder)\n",
    "    ave_std_dev_pd = pd.DataFrame([{'simulation':simulation_folder,\n",
    "                                    'pciloss': extract_hyp['pciloss'],\n",
    "                                    'ilmmi': extract_hyp['ilmmi'],\n",
    "                                    'imb': extract_hyp['imb'],\n",
    "                                    'ilcross': extract_hyp['ilcross'],\n",
    "                                    'crosstalk': extract_hyp['crosstalk'],\n",
    "                                    'fidelities': max_lastLoss_s.tolist(),}])\n",
    "    return ave_std_dev_pd\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "# MAIN\n",
    "# =================================================================================================================\n",
    "data_df = pd.DataFrame()\n",
    "for filename in os.listdir(directory_run):\n",
    "    directory_path = os.path.join(directory_run, filename)       # Construct the full file path\n",
    "    if \"n\"+str(data[\"n_inputs\"]) in directory_path:             # I don't like it but ok\n",
    "        if not os.path.isfile(directory_path):\n",
    "            fidelity_pd = extract_fidelities_1sim(directory_path, data[\"arct\"])\n",
    "            print(\"Number of data saved:\", fidelity_pd.shape[0])\n",
    "            if fidelity_pd.index.tolist() != []:   # No data in that simulation\n",
    "                simulation_folder = os.path.basename(directory_path)\n",
    "                ave_std_dev_pd = models_repmax_fidelities(fidelity_pd, simulation_folder)\n",
    "                data_df = pd.concat([data_df, ave_std_dev_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write inside the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "    # !!!!!!!!!!!! I'm APPENDING ELEMENTS !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    append_output_simulation(\n",
    "        data=data,\n",
    "        PC_i_loss_const=row['pciloss'],\n",
    "        il_mmi_const=row['ilmmi'],\n",
    "        imbalance_const=row['imb'],\n",
    "        il_cross_const=row['ilcross'],\n",
    "        cross_talk_const=row['crosstalk'],\n",
    "        fidelities=row['fidelities'],)\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by imb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary by PC_i_loss and imbalances\n",
    "\n",
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file) \n",
    "\n",
    "# Sort by CROSS TALK\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['cross_talk_const'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "# Sort by INSERSION LOSS CROSSING\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['il_cross_const'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "# Sort by IMBALANCES\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['imbalance_const'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "# Sort by INSERSION LOSS MMI\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['il_mmi_const'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "# Sort by PC_LOSSES\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['PC_i_loss_const'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_NP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

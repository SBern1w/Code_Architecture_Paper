{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eebe3c1",
   "metadata": {},
   "source": [
    "# Test my shit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0725efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================================\n",
    "# Goal: Test the quantization on the architectures\n",
    "# Check:\n",
    "\n",
    "# TODO: Add the quantization\n",
    "\n",
    "# =================================================================================================================\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.unitary_matrix_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0c2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_index = 0\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed01f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================================\n",
    "# =========================================== HYPARAMETERS ========================================================\n",
    "# =================================================================================================================\n",
    "# Each CPU run 10 repetitions and 10 different matrices\n",
    "n_CPU_X_sim = 100\n",
    "n_matrix_x_CPU = 10\n",
    "n_repetitions = 10\n",
    "\n",
    "configs = [\n",
    "    {\"index\": 0, \"model_obj\": Clements_Arct, \"N_bits\": 16},\n",
    "    {\"index\": 1, \"model_obj\": Clements_Arct, \"N_bits\": 14},\n",
    "    {\"index\": 2, \"model_obj\": Clements_Arct, \"N_bits\": 12},\n",
    "    {\"index\": 3, \"model_obj\": Clements_Arct, \"N_bits\": 10},\n",
    "    {\"index\": 4, \"model_obj\": Clements_Arct, \"N_bits\": 8},\n",
    "    {\"index\": 5, \"model_obj\": Clements_Arct, \"N_bits\": 6},\n",
    "    {\"index\": 6, \"model_obj\": Clements_Arct, \"N_bits\": 4},\n",
    "\n",
    "    {\"index\": 7, \"model_obj\": Fldzhyan_Arct, \"N_bits\": 16},\n",
    "    {\"index\": 8, \"model_obj\": Fldzhyan_Arct, \"N_bits\": 14},\n",
    "    {\"index\": 9, \"model_obj\": Fldzhyan_Arct, \"N_bits\": 12},\n",
    "    {\"index\": 10, \"model_obj\": Fldzhyan_Arct, \"N_bits\": 10},\n",
    "    {\"index\": 11, \"model_obj\": Fldzhyan_Arct, \"N_bits\": 8},\n",
    "    {\"index\": 12, \"model_obj\": Fldzhyan_Arct, \"N_bits\": 6},\n",
    "    {\"index\": 13, \"model_obj\": Fldzhyan_Arct, \"N_bits\": 4},\n",
    "\n",
    "    {\"index\": 14, \"model_obj\": NEUROPULSCrossingSide_Arct, \"N_bits\": 16},\n",
    "    {\"index\": 15, \"model_obj\": NEUROPULSCrossingSide_Arct, \"N_bits\": 14},\n",
    "    {\"index\": 16, \"model_obj\": NEUROPULSCrossingSide_Arct, \"N_bits\": 12},\n",
    "    {\"index\": 17, \"model_obj\": NEUROPULSCrossingSide_Arct, \"N_bits\": 10},\n",
    "    {\"index\": 18, \"model_obj\": NEUROPULSCrossingSide_Arct, \"N_bits\": 8},\n",
    "    {\"index\": 19, \"model_obj\": NEUROPULSCrossingSide_Arct, \"N_bits\": 6},\n",
    "    {\"index\": 20, \"model_obj\": NEUROPULSCrossingSide_Arct, \"N_bits\": 4},\n",
    "]\n",
    "\n",
    "# Each CPU\n",
    "search_index = run_index // n_CPU_X_sim\n",
    "config = next(c for c in configs if c[\"index\"] == search_index)\n",
    "model_obj = config[\"model_obj\"]\n",
    "num_folder = config[\"index\"]\n",
    "N_bits = config[\"N_bits\"]\n",
    "\n",
    "# =================================================================================================================\n",
    "n_inputs = 8\n",
    "\n",
    "lr = 0.001\n",
    "if n_inputs == 4:\n",
    "    n_epochs = 20000\n",
    "elif n_inputs == 6:\n",
    "    n_epochs = 21000\n",
    "elif n_inputs == 8:\n",
    "    n_epochs = 22000\n",
    "elif n_inputs == 10:\n",
    "    n_epochs = 23000\n",
    "elif n_inputs == 12:\n",
    "    n_epochs = 24000\n",
    "elif n_inputs == 14:\n",
    "    n_epochs = 25000\n",
    "elif n_inputs == 16:\n",
    "    n_epochs = 26000\n",
    "\n",
    "# GAUSSIAN DISTRIBUTION\n",
    "pc_iloss_mu = 0.        # Average =P_out/P_in. 0dB pefect component, -100dB very lossy\n",
    "pc_iloss_sigma = 0.     # Std deviation\n",
    "\n",
    "i_loss_MMI_mu = 0.          # Average =P_out/P_in. 0dB pefect component, -100dB very lossy\n",
    "i_loss_MMI_sigma = 0.       # Std deviation\n",
    "imbalance_mu = 0.           # Average =P_outmax/P_outmin. 0dB 50/50 MMI, 100dB all power to outUP, -100dB all power to outDOWN\n",
    "imbalance_sigma = 0.        # Std deviation\n",
    "\n",
    "i_loss_Crossing_mu = 0.         # Average =P_out/P_in. 0dB pefect component, -100dB very lossy\n",
    "i_loss_Crossing_sigma = 0.      # Std deviation\n",
    "cross_talk_mu = -1000.          # Average =P_leakout/P_otherout. -infdB Crossing perfect, -1dB very bad device a lot power leak\n",
    "cross_talk_sigma = 0.           # Std deviation\n",
    "# =================================================================================================================\n",
    "# =================================================================================================================\n",
    "# ================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5e2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the right target -------------------------------------------------------------------------------------------\n",
    "def load_targets(index_matrix):\n",
    "    all_target = torch.load(\"./dataset/targets_nIN8_nM1000.pt\")\n",
    "    target_matricies = all_target[index_matrix*n_matrix_x_CPU:(index_matrix+1)*n_matrix_x_CPU, : , :]\n",
    "    return target_matricies\n",
    "\n",
    "# Select model ----------------------------------------------------------------------------------------------------\n",
    "# Truncate the gaussian distribution\n",
    "def create_truncated_gaussian_tensor(mu, sigma, shape, max_value=None):\n",
    "    tensor = torch.normal(mean=mu, std=sigma, size=shape)\n",
    "    if max_value is not None:\n",
    "        tensor = torch.clamp(tensor, max=max_value)\n",
    "    return tensor\n",
    "\n",
    "def select_model(name_model):\n",
    "    pc_i_losses_mtx_even = create_truncated_gaussian_tensor(pc_iloss_mu, pc_iloss_sigma, (2*(n_inputs-1), n_inputs), 0)\n",
    "    pc_i_losses_mtx_even = 10**(pc_i_losses_mtx_even/10)\n",
    "    pc_i_losses_mtx_odd = create_truncated_gaussian_tensor(pc_iloss_mu, pc_iloss_sigma, (n_inputs, n_inputs), 0)\n",
    "    pc_i_losses_mtx_odd = 10**(pc_i_losses_mtx_odd/10)\n",
    "    pc_i_losses_mtx_inout = create_truncated_gaussian_tensor(pc_iloss_mu, pc_iloss_sigma, (2, n_inputs), 0)\n",
    "    pc_i_losses_mtx_inout = 10**(pc_i_losses_mtx_inout/10)\n",
    "    pc_i_losses_mtx_full = create_truncated_gaussian_tensor(pc_iloss_mu, pc_iloss_sigma, (n_inputs, n_inputs), 0)\n",
    "    pc_i_losses_mtx_full = 10**(pc_i_losses_mtx_full/10)\n",
    "    pc_i_losses_mtx_side = create_truncated_gaussian_tensor(pc_iloss_mu, pc_iloss_sigma, (n_inputs-2, n_inputs), 0)\n",
    "    pc_i_losses_mtx_side = 10**(pc_i_losses_mtx_side/10)\n",
    "\n",
    "    mmi_i_losses_mtx_even = create_truncated_gaussian_tensor(i_loss_MMI_mu, i_loss_MMI_sigma, (2*(n_inputs-1), n_inputs//2), 0)\n",
    "    mmi_i_losses_mtx_even = 10**(mmi_i_losses_mtx_even/10)\n",
    "    mmi_i_losses_mtx_odd = create_truncated_gaussian_tensor(i_loss_MMI_mu, i_loss_MMI_sigma, (n_inputs, n_inputs//2-1), 0)\n",
    "    mmi_i_losses_mtx_odd = 10**(mmi_i_losses_mtx_odd/10)\n",
    "    mmi_imbalances_mtx_even = create_truncated_gaussian_tensor(imbalance_mu, imbalance_sigma, (2*(n_inputs-1), n_inputs//2))\n",
    "    mmi_imbalances_mtx_even = 10**(mmi_imbalances_mtx_even/10)\n",
    "    mmi_imbalances_mtx_odd = create_truncated_gaussian_tensor(imbalance_mu, imbalance_sigma, (n_inputs, n_inputs//2-1))\n",
    "    mmi_imbalances_mtx_odd = 10**(mmi_imbalances_mtx_odd/10)\n",
    "\n",
    "    crossing_i_losses_mtx_odd = create_truncated_gaussian_tensor(i_loss_Crossing_mu, i_loss_Crossing_sigma, (n_inputs-2, n_inputs//2-1), 0)\n",
    "    crossing_i_losses_mtx_odd = 10**(crossing_i_losses_mtx_odd/10)\n",
    "    crossing_i_losses_mtx_odd_side = create_truncated_gaussian_tensor(i_loss_Crossing_mu, i_loss_Crossing_sigma, (n_inputs-2, n_inputs//2+1), 0)\n",
    "    crossing_i_losses_mtx_odd_side = 10**(crossing_i_losses_mtx_odd_side/10)\n",
    "    crossing_crosstalks_mtx_odd = create_truncated_gaussian_tensor(cross_talk_mu, cross_talk_sigma, (n_inputs-2, n_inputs//2-1))\n",
    "    crossing_crosstalks_mtx_odd = 10**(crossing_crosstalks_mtx_odd/10)\n",
    "    crossing_crosstalks_mtx_odd_side = create_truncated_gaussian_tensor(cross_talk_mu, cross_talk_sigma, (n_inputs-2, n_inputs//2+1))\n",
    "    crossing_crosstalks_mtx_odd_side = 10**(crossing_crosstalks_mtx_odd_side/10)\n",
    "\n",
    "    if name_model == Clements_Arct:\n",
    "        model = Clements_Arct(\n",
    "            n_inputs=n_inputs,\n",
    "            pc_i_losses_mtx_even=pc_i_losses_mtx_even,\n",
    "            pc_i_losses_mtx_odd=pc_i_losses_mtx_odd,\n",
    "            pc_i_losses_mtx_inout=pc_i_losses_mtx_inout,\n",
    "            mmi_i_losses_mtx_even=mmi_i_losses_mtx_even,\n",
    "            mmi_i_losses_mtx_odd=mmi_i_losses_mtx_odd,\n",
    "            mmi_imbalances_mtx_even=mmi_imbalances_mtx_even,\n",
    "            mmi_imbalances_mtx_odd=mmi_imbalances_mtx_odd,\n",
    "            N_bits=N_bits)\n",
    "    elif name_model == Fldzhyan_Arct:\n",
    "        model = Fldzhyan_Arct(\n",
    "            n_inputs=n_inputs,\n",
    "            pc_i_losses_mtx_even=pc_i_losses_mtx_even,\n",
    "            pc_i_losses_mtx_odd=pc_i_losses_mtx_odd,\n",
    "            pc_i_losses_mtx_inout=pc_i_losses_mtx_inout,\n",
    "            mmi_i_losses_mtx_even=mmi_i_losses_mtx_even,\n",
    "            mmi_i_losses_mtx_odd=mmi_i_losses_mtx_odd,\n",
    "            mmi_imbalances_mtx_even=mmi_imbalances_mtx_even,\n",
    "            mmi_imbalances_mtx_odd=mmi_imbalances_mtx_odd,\n",
    "            N_bits=N_bits)\n",
    "    elif name_model == NEUROPULSCrossingSide_Arct:\n",
    "        model = NEUROPULSCrossingSide_Arct(\n",
    "            n_inputs=n_inputs,\n",
    "            pc_i_losses_mtx_even=pc_i_losses_mtx_even,\n",
    "            pc_i_losses_mtx_inout=pc_i_losses_mtx_even,\n",
    "            mmi_i_losses_mtx_even=mmi_i_losses_mtx_even,\n",
    "            mmi_imbalances_mtx_even=mmi_imbalances_mtx_even,\n",
    "            crossing_i_losses_mtx_odd=crossing_i_losses_mtx_odd_side,\n",
    "            crossing_crosstalks_mtx_odd=crossing_crosstalks_mtx_odd_side,\n",
    "            N_bits=N_bits)\n",
    "    else:\n",
    "        model = None\n",
    "        raise Exception('Something not good on the input')\n",
    "    return model\n",
    "\n",
    "# Fidelity and Loss function --------------------------------------------------------------------------------------\n",
    "def FidelityUnitary(predicted_matrix, target_matrix):\n",
    "    n_inputs = predicted_matrix.shape[0]\n",
    "    predicted_matrix = predicted_matrix.to(torch.complex128)\n",
    "    target_matrix = target_matrix.to(torch.complex128)\n",
    "    Frobenius_module_p = torch.trace(torch.matmul(predicted_matrix.t().conj(), predicted_matrix))\n",
    "    Frobenius_pt = torch.trace(torch.matmul(predicted_matrix.t().conj(), target_matrix))\n",
    "    cosine_similarity = (torch.abs(Frobenius_pt))**2/(n_inputs*Frobenius_module_p)\n",
    "    Fidelity = torch.abs(cosine_similarity)\n",
    "    return Fidelity\n",
    "\n",
    "def Loss_FildelityUnitary(predicted_matrix, target_matrix):\n",
    "    fidelity = FidelityUnitary(predicted_matrix, target_matrix)\n",
    "    return 1 - fidelity\n",
    "\n",
    "# Calculate the prediction ----------------------------------------------------------------------------------------\n",
    "def model_training(model, optimizer, target_matrix):\n",
    "    for _ in range(n_epochs):     # Optimiziation with gradient\n",
    "        pred_matrix = model()\n",
    "        loss = Loss_FildelityUnitary(pred_matrix, target_matrix)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Get fidelity\n",
    "    with torch.no_grad():\n",
    "        pred_matrix = model()\n",
    "        fidelity = FidelityUnitary(pred_matrix, target_matrix)\n",
    "    return fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afdd8cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [18:27<2:46:06, 1107.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m         model \u001b[38;5;241m=\u001b[39m select_model(model_obj)\n\u001b[0;32m     16\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m---> 17\u001b[0m         last_fidelity \u001b[38;5;241m=\u001b[39m model_training(model, optimizer, targets[idx_targ, : , :])\n\u001b[0;32m     18\u001b[0m         fidelities\u001b[38;5;241m.\u001b[39mappend(last_fidelity)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Save the results model ======================================================================================\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create folder and retun the directory:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 104\u001b[0m, in \u001b[0;36mmodel_training\u001b[1;34m(model, optimizer, target_matrix)\u001b[0m\n\u001b[0;32m    102\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    103\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 104\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Get fidelity\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\femarche\\.conda\\envs\\torch_NP2\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\femarche\\.conda\\envs\\torch_NP2\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\femarche\\.conda\\envs\\torch_NP2\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\femarche\\.conda\\envs\\torch_NP2\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\femarche\\.conda\\envs\\torch_NP2\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =================================================================================================================\n",
    "# =============================================== MAIN ============================================================\n",
    "# =================================================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the right data\n",
    "    index_matrix = run_index % n_CPU_X_sim\n",
    "    targets = load_targets(index_matrix)\n",
    "\n",
    "    # Simulations\n",
    "    fidelities = []\n",
    "    n_targets = targets.shape[0]\n",
    "    for idx_targ in tqdm.trange(n_targets):\n",
    "        for rep in range(n_repetitions):\n",
    "            # Initialize new model with different initial phase shifts for each simulaiton\n",
    "            model = select_model(model_obj)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            last_fidelity = model_training(model, optimizer, targets[idx_targ, : , :])\n",
    "            fidelities.append(last_fidelity)\n",
    "    \n",
    "    # Save the results model ======================================================================================\n",
    "    # Create folder and retun the directory:\n",
    "    base_dir = \"./outdata/\"\n",
    "    # Create the new run directory\n",
    "    run_dir = os.path.join(base_dir, f'run{num_folder}')\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    # Create the full path\n",
    "    filename = f'simulation_{run_index}.pt'\n",
    "    full_path = os.path.join(run_dir, filename)\n",
    "    # Create a dictionary\n",
    "    save_dict = {\n",
    "        'model_name': model_obj.__name__,\n",
    "        'run_index': run_index,\n",
    "        'fidelities': fidelities,\n",
    "    }\n",
    "    # Save it\n",
    "    torch.save(save_dict, full_path)\n",
    "\n",
    "    end_time = time.time()\n",
    "    work_duration = end_time - start_time\n",
    "    max_duration_human_readable = str(timedelta(seconds=work_duration))\n",
    "    print(f\"The maximum work duration is {max_duration_human_readable} (HH:MM:SS).\")\n",
    "    \n",
    "    print(\"Yeeeeh the code has finished!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d1b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_NP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

from mpi4py import MPI
import numpy as np

def compute(data):
    # Example computation: square the input
    return data * data

def main():
    comm = MPI.COMM_WORLD       # defines a group of processes that can communicate with each other.
    rank = comm.Get_rank()      # variable represents the rank of the current process within the communicato comm
    size = comm.Get_size()      # represents the size of the communicator comm

    data = None
    if rank == 0:
        data = 100  # This could be any data generated by process 0
        print(f"Process {rank} is broadcasting data: {data}")

    # Broadcast data from process 0 to all processes
    data = comm.bcast(data, root=0)

    # Each process performs the computation
    result = compute(data)
    print(f"Process {rank} computed result: {result}")

    # Gather all results back to process 0
    all_results = comm.gather(result, root=0)

    # Process 0 could then save or further process the data
    if rank == 0:
        print(f"All results gathered at process 0: {all_results}")
    
def example1():
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    if rank == 0:
        # Create data on rank 0
        data = np.arange(size)  # Create a vector with elements from 0 to size-1
    else:
        data = None  # Placeholder for data on other ranks

    # Allocate space to receive scattered data
    local_data = np.empty(1, dtype=int)

    # Scatter data from rank 0 to all other ranks
    comm.Scatter(data, local_data, root=0)

    # Print data on each rank
    print("Rank:", rank, "Data:", local_data[0])

    gathered_data = comm.gather(local_data, root=0)
    
    if rank == 0:
        print("Gathered Data on Rank 0:", gathered_data)

if __name__ == "__main__":
    example1()
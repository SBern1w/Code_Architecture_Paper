{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow to create the good data organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data INPUT to be written to a JSON file\n",
    "\n",
    "directory_run = 'outdata/20240723_run_3_gaus'\n",
    "\n",
    "name_architectures = [\n",
    "    'Clements_Arct', 'ClementsBell_Arct', 'Fldzhyan_Arct', 'FldzhyanBell_Arct',\n",
    "    'FldzhyanBellHalf_Arct',\n",
    "    'NEUROPULS_Arct', 'NEUROPULSCrossingSide_Arct', 'NEUROPULSBell_Arct', 'NEUROPULSBellCrossingSide_Arct', 'NEUROPULSBellCrossingSide2_Arct',\n",
    "    'NEUROPULSHalf_Arct', 'NEUROPULSBellHalf_Arct', 'NEUROPULSBellHalfCrossingSide_Arct']\n",
    "\n",
    "arct = name_architectures[0]\n",
    "\n",
    "n_inputs = 8\n",
    "if n_inputs == 4:\n",
    "    n_epochs = 20000\n",
    "elif n_inputs == 6:\n",
    "    n_epochs = 21000\n",
    "elif n_inputs == 8:\n",
    "    n_epochs = 22000\n",
    "elif n_inputs == 10:\n",
    "    n_epochs = 23000\n",
    "elif n_inputs == 12:\n",
    "    n_epochs = 24000\n",
    "elif n_inputs == 14:\n",
    "    n_epochs = 25000\n",
    "elif n_inputs == 16:\n",
    "    n_epochs = 26000\n",
    "\n",
    "name_file = \"traking_\"+arct+\"_N\"+str(n_inputs)\n",
    "\n",
    "data = {\n",
    "    \"name_file\": name_file,\n",
    "    \"arct\": arct,\n",
    "    \"n_inputs\": n_inputs,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"date\": \"20240723\",\n",
    "    \"train_type\": \"1-Fidelity\",\n",
    "    \"seed\": 37,\n",
    "    \"n_matrices\": 1000,\n",
    "    \"n_repetitions\": 5,\n",
    "    \"lr\": 0.001,\n",
    "    \"n_bachup\": 500,\n",
    "\n",
    "    \"data_out_type\": \"Fidelity\",\n",
    "    \"rep_type\": \"max\",\n",
    "    \"simulations\": []\n",
    "    }\n",
    "\n",
    "folder_relative_path = \"organized_data/\"\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_gaussian_output_simulation(data, pc_iloss_mu, pc_iloss_sigma, imbalance_mu, average=None, std_dev=None):\n",
    "    dictionary = {\n",
    "        \"pc_iloss_mu\": pc_iloss_mu,\n",
    "        \"pc_iloss_sigma\": pc_iloss_sigma,\n",
    "        \"i_loss_MMI_mu\": -0.25,\n",
    "        \"i_loss_MMI_sigma\": 0.1,\n",
    "        \"imbalance_mu\": imbalance_mu,\n",
    "        \"imbalance_sigma\": 0.15,\n",
    "        \"i_loss_Crossing_mu\": -0.25,\n",
    "        \"i_loss_Crossing_sigma\": 0.05,\n",
    "        \"cross_talk_mu\": -35.0,\n",
    "        \"cross_talk_sigma\": 1.0,\n",
    "\n",
    "        \"average\": average,\n",
    "        \"std_dev\": std_dev\n",
    "        }\n",
    "    \n",
    "    data[\"simulations\"].append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive the Average and Std Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive Fidelity for the one simulation\n",
    "# Compute Fidelity Unitary\n",
    "def FidelityUnitary(target_matrix, predicted_matrix):\n",
    "    target_matrix = np.array(target_matrix, dtype=np.complex128)\n",
    "    predicted_matrix = np.array(predicted_matrix, dtype=np.complex128)\n",
    "    trace_pH_p = np.trace(np.dot(predicted_matrix.conj().T, predicted_matrix)).real    # result is REAL\n",
    "    trace_pH_t = np.trace(np.dot(predicted_matrix.conj().T, target_matrix))\n",
    "    cosine_similarity = np.abs(trace_pH_t)**2/(target_matrix.shape[0]*trace_pH_p)\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "# function get the directory and condition\n",
    "# check all the files and if its good save the\n",
    "def extract_fidelities_1sim(directory, arct):\n",
    "    fidelities = []\n",
    "    labels = []\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)       # Construct the full file path\n",
    "        if os.path.isfile(file_path):                       # Check if it is a file\n",
    "            result_sim = np.load(file_path, allow_pickle=True)\n",
    "            target_predit_tuples = [(label, array) for label, array in result_sim if arct in label]\n",
    "        if target_predit_tuples:    # If there are elements\n",
    "            labels.extend([target_predit_tuples[i][0].replace(\"target_\", \"\", 1) for i in range(0, len(target_predit_tuples), 2)])\n",
    "            fidelities.extend([FidelityUnitary(target_predit_tuples[i][1], target_predit_tuples[i+1][1]) for i in range(0, len(target_predit_tuples), 2)])\n",
    "        else:\n",
    "            return None\n",
    "    fidelity_pd = pd.DataFrame({'label': labels, 'value': fidelities})\n",
    "    return fidelity_pd\n",
    "\n",
    "\n",
    "def extract_values(simulation_folder):\n",
    "    # Patterns to match each value, ensuring correct order\n",
    "    patterns = {\n",
    "        'pcilossmu': r\"pcilossmu(-?\\d+\\.\\d+)\",\n",
    "        'pcilosssigma': r\"pcilosssigma(-?\\d+\\.\\d+)\",\n",
    "    }\n",
    "    values = {}\n",
    "    last_end = 0  # Track the end position of the last match\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, simulation_folder[last_end:])\n",
    "        if match:\n",
    "            values[key] = float(match.group(1))\n",
    "            last_end += match.end()\n",
    "    return values\n",
    "\n",
    "# take max value and derive average and std dev\n",
    "def models_repmax_mean_devstd(df, simulation_folder):\n",
    "    # Extracting the base label (e.g., 'LabelA', 'LabelB')\n",
    "    df['label_no_rep'] = df['label'].apply(lambda x: x.split('_rep')[0])\n",
    "    # Group with the base label and find max\n",
    "    max_lastLoss_s = df.groupby('label_no_rep')['value'].max()\n",
    "\n",
    "    averages = max_lastLoss_s.mean()\n",
    "    std_devs = max_lastLoss_s.std()\n",
    "    \n",
    "    # Extract and convert to float\n",
    "    extract_hyp = extract_values(simulation_folder)\n",
    "    ave_std_dev_pd = pd.DataFrame([{'simulation':simulation_folder,\n",
    "                                    'average': averages, 'std_dev': std_devs,\n",
    "                                    'pcilossmu': extract_hyp['pcilossmu'],\n",
    "                                    'pcilosssigma': extract_hyp['pcilosssigma'],}])\n",
    "    return ave_std_dev_pd\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "# MAIN\n",
    "# =================================================================================================================\n",
    "data_df = pd.DataFrame()\n",
    "for filename in os.listdir(directory_run):\n",
    "    directory_path = os.path.join(directory_run, filename)       # Construct the full file path\n",
    "    if \"n\"+str(data[\"n_inputs\"]) in directory_path:             # I don't like it but ok\n",
    "        if not os.path.isfile(directory_path):\n",
    "            fidelity_pd = extract_fidelities_1sim(directory_path, data[\"arct\"])\n",
    "            if fidelity_pd is not None:   # No data in that simulation\n",
    "                simulation_folder = os.path.basename(directory_path)\n",
    "                ave_std_dev_pd = models_repmax_mean_devstd(fidelity_pd, simulation_folder)\n",
    "                data_df = pd.concat([data_df, ave_std_dev_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write inside the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "    # !!!!!!!!!!!! I'm APPENDING ELEMENTS !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    append_gaussian_output_simulation(\n",
    "        data=data,\n",
    "        pc_iloss_mu=row['pcilossmu'],\n",
    "        pc_iloss_sigma=row['pcilosssigma'],\n",
    "        average=row['average'],\n",
    "        std_dev=row['std_dev'],)\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by imb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary by PC_i_loss and imbalances\n",
    "import json\n",
    "\n",
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file) \n",
    "\n",
    "# Sort by PhaseChanger INSERTION LOSS SIGMA\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['pc_iloss_sigma'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "# Sort by PhaseChanger INSERTION LOSS MU\n",
    "simulations = data[\"simulations\"]\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['pc_iloss_mu'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_NP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

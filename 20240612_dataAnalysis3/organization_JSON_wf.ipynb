{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow to create the good data organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Data INPUT to be written to a JSON file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m name_architectures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClements_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClementsBell_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFldzhyan_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFldzhyanBell_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFldzhyanBellHalf_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEUROPULS_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEUROPULSCrossingSide_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEUROPULSBell_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEUROPULSBell_CrossingSide_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEUROPULSHalf_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEUROPULSBellHalf_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEUROPULSBellHalfCrossingSide_Arct\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m arct \u001b[38;5;241m=\u001b[39m name_architectures[\u001b[38;5;241m12\u001b[39m]\n\u001b[0;32m     10\u001b[0m n_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_inputs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Data INPUT to be written to a JSON file\n",
    "name_architectures = [\n",
    "    'Clements_Arct', 'ClementsBell_Arct', 'Fldzhyan_Arct', 'FldzhyanBell_Arct',\n",
    "    'FldzhyanBellHalf_Arct',\n",
    "    'NEUROPULS_Arct', 'NEUROPULSCrossingSide_Arct', 'NEUROPULSBell_Arct', 'NEUROPULSBell_CrossingSide_Arct',\n",
    "    'NEUROPULSHalf_Arct', 'NEUROPULSBellHalf_Arct', 'NEUROPULSBellHalfCrossingSide_Arct']\n",
    "\n",
    "arct = name_architectures[12]\n",
    "\n",
    "n_inputs = 8\n",
    "if n_inputs == 4:\n",
    "    n_epochs = 20000\n",
    "elif n_inputs == 8:\n",
    "    n_epochs = 22000\n",
    "elif n_inputs == 16:\n",
    "    n_epochs = 25000\n",
    "\n",
    "name_file = \"traking_\"+arct+\"_N\"+str(n_inputs)\n",
    "\n",
    "data = {\n",
    "    \"name_file\": name_file,\n",
    "    \"arct\": arct,\n",
    "    \"n_inputs\": n_inputs,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"date\": \"20240619\",\n",
    "    \"train_type\": \"1-Fidelity\",\n",
    "    \"seed\": 37,\n",
    "    \"n_matrices\": 1000,\n",
    "    \"n_repetitions\": 5,\n",
    "    \"lr\": 0.001,\n",
    "    \"n_bachup\": 500,\n",
    "\n",
    "    \"data_out_type\": \"Fidelity\",\n",
    "    \"rep_type\": \"max\",\n",
    "    \"simulations\": []\n",
    "    }\n",
    "\n",
    "folder_relative_path = \"organized_data/\"\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_output_simulation(data, PC_i_loss_const, i_loss_const, imbalance_const, cross_talk_const, average=None, std_dev=None):\n",
    "    dictionary = {\n",
    "        \"PC_i_loss_const\": PC_i_loss_const,\n",
    "        \"i_loss_const\": i_loss_const,\n",
    "        \"imbalance_const\": imbalance_const,\n",
    "        \"cross_talk_const\": cross_talk_const,\n",
    "        \"average\": average,\n",
    "        \"std_dev\": std_dev\n",
    "        }\n",
    "    \n",
    "    data[\"simulations\"].append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive the Average and Std Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MSE Data analysis\n",
    "\n",
    "# # import stuff\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# # extract data of one simulation\n",
    "# def extract_targets_preditions_1sim(directory):\n",
    "#     target_tuples = []\n",
    "#     predit_tuples = []\n",
    "#     # Loop through each file in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)       # Construct the full file path\n",
    "#         if os.path.isfile(file_path):                       # Check if it is a file\n",
    "#             result_sim = np.load(file_path, allow_pickle=True)\n",
    "#             target_tuples.extend([(label, array) for label, array in result_sim if 'target' in label])\n",
    "#             predit_tuples.extend([(label, array) for label, array in result_sim if 'prediction' in label])\n",
    "#     return target_tuples, predit_tuples\n",
    "\n",
    "# # derive MSE for the one simulation\n",
    "# # Compute MSE\n",
    "# def CMatrixMSELoss(target_matrix, predicted_matrix):\n",
    "#     target_matrix = np.array(target_matrix, dtype=np.complex128)\n",
    "#     predicted_matrix = np.array(predicted_matrix, dtype=np.complex128)\n",
    "#     mag_diff_sq = np.abs(predicted_matrix - target_matrix)**2\n",
    "#     loss = np.sum(mag_diff_sq) / np.size(target_matrix)\n",
    "#     return loss\n",
    "\n",
    "# def remove_until_underscore(s):     # from the end until the first '_'\n",
    "#     parts = s.split('_')\n",
    "#     result = '_'.join(parts[1:])\n",
    "#     return result\n",
    "\n",
    "# def derive_MSE_1sim(target_tuples, predit_tuples):\n",
    "#     loss = []\n",
    "#     for i in range(len(target_tuples)):\n",
    "#         if remove_until_underscore(target_tuples[i][0]) == remove_until_underscore(predit_tuples[i][0]):\n",
    "#             loss.extend([CMatrixMSELoss(target_tuples[i][1], predit_tuples[i][1])])\n",
    "#         else:\n",
    "#             print(\"Capo abbiamo un problema :(\")\n",
    "#     labels = [t[0] for t in predit_tuples]\n",
    "#     loss_pd = pd.DataFrame({'label': labels, 'value': loss})\n",
    "#     return loss_pd\n",
    "\n",
    "# # take min value and derive average and std dev\n",
    "# def models_repmin_mean_devstd(df, name_models, simulation_folder):\n",
    "#     # Extracting the base label (e.g., 'LabelA', 'LabelB')\n",
    "#     df['label_no_rep'] = df['label'].apply(lambda x: x.split('_rep')[0])\n",
    "#     # Group with the base label and find min\n",
    "#     min_lastLoss_s = df.groupby('label_no_rep')['value'].min()\n",
    "\n",
    "#     averages = []\n",
    "#     std_devs = []\n",
    "#     for name in name_models:\n",
    "#         # model_s take the all 1000 sim same model with the best repetition value\n",
    "#         model_s = min_lastLoss_s[min_lastLoss_s.index.str.contains(name)]\n",
    "#         averages.append(model_s.mean())\n",
    "#         std_devs.append(model_s.std())\n",
    "    \n",
    "#     # Extract and convert to float\n",
    "#     iloss_match = re.search(r\"iloss(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "#     imb_match = re.search(r\"imb(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "#     iloss = float(iloss_match.group(1)) if iloss_match else None\n",
    "#     imb = float(imb_match.group(1)) if imb_match else None\n",
    "#     ave_std_dev_pd = pd.DataFrame({'simulation':simulation_folder, 'name_model': name_models,\n",
    "#                                    'average': averages, 'std_dev': std_devs,\n",
    "#                                    'iloss': iloss, 'imb': imb,})\n",
    "#     return ave_std_dev_pd\n",
    "\n",
    "\n",
    "# # =================================================================================================================\n",
    "# # MAIN\n",
    "# # =================================================================================================================\n",
    "# directory_run = 'outdata/20240610_idealCrossing'\n",
    "# name_models = [\n",
    "#     'Clements_Arct', 'ClementsBell_Arct', 'Fldzhyan_Arct', 'FldzhyanBell_Arct',\n",
    "#     'FldzhyanBellHalf_Arct',\n",
    "#     'NEUROPULS_Arct', 'NEUROPULSCrossingSide_Arct', 'NEUROPULSBell_Arct', 'NEUROPULSBell_CrossingSide_Arct',\n",
    "#     'NEUROPULSHalf_Arct', 'NEUROPULSBellHalf_Arct', 'NEUROPULSBellHalfCrossingSide_Arct']\n",
    "\n",
    "# data_df = pd.DataFrame()\n",
    "# for filename in os.listdir(directory_run):\n",
    "#     directory_path = os.path.join(directory_run, filename)       # Construct the full file path\n",
    "#     if \"n\"+str(data[\"n_inputs\"]) in directory_path:     # take the n I wanted\n",
    "#         if not os.path.isfile(directory_path):\n",
    "#             target_tuples, predit_tuples = extract_targets_preditions_1sim(directory_path)\n",
    "#             mse_pd = derive_MSE_1sim(target_tuples, predit_tuples)\n",
    "#             simulation_folder = os.path.basename(directory_path)\n",
    "#             ave_std_dev_pd = models_repmin_mean_devstd(mse_pd, name_models, simulation_folder)\n",
    "#             data_df = pd.concat([data_df, ave_std_dev_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fidelity Data analysis\n",
    "\n",
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# extract data of one simulation\n",
    "def extract_targets_preditions_1sim(directory, arct):\n",
    "    target_tuples = []\n",
    "    predit_tuples = []\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)       # Construct the full file path\n",
    "        if os.path.isfile(file_path):                       # Check if it is a file\n",
    "            result_sim = np.load(file_path, allow_pickle=True)\n",
    "            target_tuples.extend([(label, array) for label, array in result_sim if 'target' in label and arct in label])\n",
    "            predit_tuples.extend([(label, array) for label, array in result_sim if 'prediction' in label and arct in label])\n",
    "    return target_tuples, predit_tuples\n",
    "\n",
    "# derive Fidelity for the one simulation\n",
    "# Compute Fidelity Unitary\n",
    "def FidelityUnitary(target_matrix, predicted_matrix):\n",
    "    target_matrix = np.array(target_matrix, dtype=np.complex128)\n",
    "    predicted_matrix = np.array(predicted_matrix, dtype=np.complex128)\n",
    "    trace_pH_p = np.trace(np.dot(predicted_matrix.conj().T, predicted_matrix)).real    # result is REAL\n",
    "    trace_pH_t = np.trace(np.dot(predicted_matrix.conj().T, target_matrix))\n",
    "    cosine_similarity = np.abs(trace_pH_t)**2/(target_matrix.shape[0]*trace_pH_p)\n",
    "    return cosine_similarity\n",
    "\n",
    "def remove_until_underscore(s):     # from the end until the first '_'\n",
    "    parts = s.split('_')\n",
    "    result = '_'.join(parts[1:])\n",
    "    return result\n",
    "\n",
    "def derive_Fidelity_1sim(target_tuples, predit_tuples):\n",
    "    loss = []\n",
    "    for i in range(len(target_tuples)):\n",
    "        if remove_until_underscore(target_tuples[i][0]) == remove_until_underscore(predit_tuples[i][0]):\n",
    "            loss.extend([FidelityUnitary(target_tuples[i][1], predit_tuples[i][1])])\n",
    "        else:\n",
    "            print(\"Capo abbiamo un problema :(\")\n",
    "    labels = [t[0] for t in predit_tuples]\n",
    "    loss_pd = pd.DataFrame({'label': labels, 'value': loss})\n",
    "    return loss_pd\n",
    "\n",
    "# take max value and derive average and std dev\n",
    "def models_repmax_mean_devstd(df, simulation_folder):\n",
    "    # Extracting the base label (e.g., 'LabelA', 'LabelB')\n",
    "    df['label_no_rep'] = df['label'].apply(lambda x: x.split('_rep')[0])\n",
    "    # Group with the base label and find max\n",
    "    max_lastLoss_s = df.groupby('label_no_rep')['value'].max()\n",
    "\n",
    "    averages = max_lastLoss_s.mean()\n",
    "    std_devs = max_lastLoss_s.std()\n",
    "    \n",
    "    # Extract and convert to float\n",
    "    pciloss_match = re.search(r\"pciloss(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "    iloss_match = re.search(r\"iloss(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "    imb_match = re.search(r\"imb(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "    crosstalk_match = re.search(r\"crosstalk(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "\n",
    "    pciloss = float(pciloss_match.group(1)) if pciloss_match else None\n",
    "    iloss = float(iloss_match.group(1)) if iloss_match else None\n",
    "    imb = float(imb_match.group(1)) if imb_match else None\n",
    "    crosstalk = float(crosstalk_match.group(1)) if crosstalk_match else None\n",
    "    ave_std_dev_pd = pd.DataFrame([{'simulation':simulation_folder,\n",
    "                                   'average': averages, 'std_dev': std_devs,\n",
    "                                   'pciloss': pciloss, 'iloss': iloss,\n",
    "                                   'imb': imb, 'crosstalk': crosstalk,}])\n",
    "    return ave_std_dev_pd\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "# MAIN\n",
    "# =================================================================================================================\n",
    "directory_run = 'outdata/20240619_allDataFidelity_IMPORTANT'\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "for filename in os.listdir(directory_run):\n",
    "    directory_path = os.path.join(directory_run, filename)       # Construct the full file path\n",
    "    if \"n\"+str(data[\"n_inputs\"]) in directory_path:     # take the n I wanted\n",
    "        if not os.path.isfile(directory_path):\n",
    "            target_tuples, predit_tuples = extract_targets_preditions_1sim(directory_path, data[\"arct\"])\n",
    "            if target_tuples != []:\n",
    "                fidelity_pd = derive_Fidelity_1sim(target_tuples, predit_tuples)\n",
    "                simulation_folder = os.path.basename(directory_path)\n",
    "                ave_std_dev_pd = models_repmax_mean_devstd(fidelity_pd, simulation_folder)\n",
    "                data_df = pd.concat([data_df, ave_std_dev_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write inside the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\femarche\\AppData\\Local\\Temp\\ipykernel_30864\\176496115.py:13: RuntimeWarning: divide by zero encountered in log10\n",
      "  cross_talk_const=10*np.log10(row['crosstalk']/(1-row['crosstalk'])),        # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "\n",
    "    # !!!!!!!!!!!! I'm APPENDING ELEMENTS !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    append_output_simulation(\n",
    "        data=data,\n",
    "        PC_i_loss_const=row['pciloss'],\n",
    "        i_loss_const=10*np.log10(1-row['iloss']),                               # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        imbalance_const=10*np.log10((1/2+row['imb'])/(1/2-row['imb'])),         # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        cross_talk_const=10*np.log10(row['crosstalk']/(1+row['crosstalk'])),        # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        average=row['average'],\n",
    "        std_dev=row['std_dev'],)\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by imb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary by architecture and imbalances\n",
    "import json\n",
    "\n",
    "# architecture_order = [\n",
    "#     'Clements_Arct', 'ClementsBell_Arct', 'Fldzhyan_Arct', 'FldzhyanBell_Arct',\n",
    "#     'FldzhyanBellHalf_Arct',\n",
    "#     'NEUROPULS_Arct', 'NEUROPULSCrossingSide_Arct', 'NEUROPULSBell_Arct', 'NEUROPULSBell_CrossingSide_Arct',\n",
    "#     'NEUROPULSHalf_Arct', 'NEUROPULSBellHalf_Arct', 'NEUROPULSBellHalfCrossingSide_Arct']\n",
    "\n",
    "# # Sorting function\n",
    "# def custom_sort_key(item):\n",
    "#     return (architecture_order.index(item['architecture']), item['imbalance_const'])\n",
    "\n",
    "# Read\n",
    "with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'r') as file:\n",
    "    data = json.load(file) \n",
    "\n",
    "simulations = data[\"simulations\"]\n",
    "# Sort the list\n",
    "sorted_simulations = sorted(simulations, key=lambda x: x['imbalance_const'])\n",
    "data[\"simulations\"] = sorted_simulations\n",
    "\n",
    "if True:\n",
    "    # Write JSON data to a file\n",
    "    with open(folder_relative_path+data[\"date\"]+\"_\"+data[\"name_file\"]+'.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORST CASE Fidelity min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fidelity Data analysis\n",
    "\n",
    "# import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# extract data of one simulation\n",
    "def extract_targets_preditions_1sim(directory):\n",
    "    target_tuples = []\n",
    "    predit_tuples = []\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)       # Construct the full file path\n",
    "        if os.path.isfile(file_path):                       # Check if it is a file\n",
    "            result_sim = np.load(file_path, allow_pickle=True)\n",
    "            target_tuples.extend([(label, array) for label, array in result_sim if 'target' in label])\n",
    "            predit_tuples.extend([(label, array) for label, array in result_sim if 'prediction' in label])\n",
    "    return target_tuples, predit_tuples\n",
    "\n",
    "# derive Fidelity for the one simulation\n",
    "# Compute Fidelity Unitary\n",
    "def FidelityUnitary(target_matrix, predicted_matrix):\n",
    "    target_matrix = np.array(target_matrix, dtype=np.complex128)\n",
    "    predicted_matrix = np.array(predicted_matrix, dtype=np.complex128)\n",
    "    trace_pH_p = np.trace(np.dot(predicted_matrix.conj().T, predicted_matrix)).real    # result is REAL\n",
    "    trace_pH_t = np.trace(np.dot(predicted_matrix.conj().T, target_matrix))\n",
    "    cosine_similarity = np.abs(trace_pH_t)**2/(target_matrix.shape[0]*trace_pH_p)\n",
    "    return cosine_similarity\n",
    "\n",
    "def remove_until_underscore(s):     # from the end until the first '_'\n",
    "    parts = s.split('_')\n",
    "    result = '_'.join(parts[1:])\n",
    "    return result\n",
    "\n",
    "def derive_Fidelity_1sim(target_tuples, predit_tuples):\n",
    "    loss = []\n",
    "    for i in range(len(target_tuples)):\n",
    "        if remove_until_underscore(target_tuples[i][0]) == remove_until_underscore(predit_tuples[i][0]):\n",
    "            loss.extend([FidelityUnitary(target_tuples[i][1], predit_tuples[i][1])])\n",
    "        else:\n",
    "            print(\"Capo abbiamo un problema :(\")\n",
    "    labels = [t[0] for t in predit_tuples]\n",
    "    loss_pd = pd.DataFrame({'label': labels, 'value': loss})\n",
    "    return loss_pd\n",
    "\n",
    "# take min value and derive average and std dev\n",
    "def models_repmin_mean_devstd(df, name_models, simulation_folder):\n",
    "    # Extracting the base label (e.g., 'LabelA', 'LabelB')\n",
    "    df['label_no_rep'] = df['label'].apply(lambda x: x.split('_rep')[0])\n",
    "    # Group with the base label and find min\n",
    "    min_lastLoss_s = df.groupby('label_no_rep')['value'].min()\n",
    "\n",
    "    averages = []\n",
    "    std_devs = []\n",
    "    for name in name_models:\n",
    "        # model_s take the all 1000 sim same model with the best repetition value\n",
    "        model_s = min_lastLoss_s[min_lastLoss_s.index.str.contains(name)]\n",
    "        averages.append(model_s.mean())\n",
    "        std_devs.append(model_s.std())\n",
    "    \n",
    "    # Extract and convert to float\n",
    "    iloss_match = re.search(r\"iloss(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "    imb_match = re.search(r\"imb(-?\\d+\\.\\d+)\", simulation_folder)\n",
    "    iloss = float(iloss_match.group(1)) if iloss_match else None\n",
    "    imb = float(imb_match.group(1)) if imb_match else None\n",
    "    ave_std_dev_pd = pd.DataFrame({'simulation':simulation_folder, 'name_model': name_models,\n",
    "                                   'average': averages, 'std_dev': std_devs,\n",
    "                                   'iloss': iloss, 'imb': imb,})\n",
    "    return ave_std_dev_pd\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "# MAIN\n",
    "# =================================================================================================================\n",
    "directory_run = 'outdata/20240612_fidelity_n4_iloss0.4'\n",
    "name_models = [\n",
    "    'Clements_Arct', 'ClementsBell_Arct', 'Fldzhyan_Arct', 'FldzhyanBell_Arct',\n",
    "    'FldzhyanBellHalf_Arct',\n",
    "    'NEUROPULS_Arct', 'NEUROPULSCrossingSide_Arct', 'NEUROPULSBell_Arct', 'NEUROPULSBell_CrossingSide_Arct',\n",
    "    'NEUROPULSHalf_Arct', 'NEUROPULSBellHalf_Arct', 'NEUROPULSBellHalfCrossingSide_Arct']\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "for filename in os.listdir(directory_run):\n",
    "    directory_path = os.path.join(directory_run, filename)       # Construct the full file path\n",
    "    if \"n\"+str(data[\"n_inputs\"]) in directory_path:     # take the n I wanted\n",
    "        if not os.path.isfile(directory_path):\n",
    "            target_tuples, predit_tuples = extract_targets_preditions_1sim(directory_path)\n",
    "            mse_pd = derive_Fidelity_1sim(target_tuples, predit_tuples)\n",
    "            simulation_folder = os.path.basename(directory_path)\n",
    "            ave_std_dev_pd = models_repmin_mean_devstd(mse_pd, name_models, simulation_folder)\n",
    "            data_df = pd.concat([data_df, ave_std_dev_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_NP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
